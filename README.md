# WR-AI: Industrial AI Integration POC

![WR-AI Dashboard](WR-AI.png)

**WR-AI** is an intelligent "smart layer" designed to monitor, diagnose, and optimize industrial machines. The POC now delivers three working modules end-to-end: OEE foundation, AI anomaly detection (historical view), and guided diagnosis with per-anomaly chat memory.

## ðŸŽ¥ Demo Video
Watch the interface in action:

[![WR-AI Interface Demo](https://img.youtube.com/vi/FvLEpaICaD8/0.jpg)](https://youtu.be/FvLEpaICaD8)

## ðŸš€ Whatâ€™s Included Today
- **Module 1 â€“ Foundation & OEE**: Real-time machine status, OEE (Availability/Performance/Quality), energy analytics, Pareto of downtime causes.
- **Module 2 â€“ Anomaly Detection**: Isolation Forest risk scoring on vibration/temperature/power; historical charts with clickable anomalies; debounced alerts.
- **Module 3 â€“ Guided Diagnosis (LLM)**: Dual LLM (Ollama default, Gemini optional), manual/RAG context, anomaly context injection, chat UI, and per-anomaly chat history saved to SQLite.

## ðŸ›  Architecture & Stack
- **Backend**: FastAPI, modular (foundation, anomaly_detection, guided_diagnosis), IsolationForest, logging, SQLite persistence at [backend/modules/anomaly_detection/anomaly_data.db](backend/modules/anomaly_detection/anomaly_data.db).
- **Frontend**: React (Vite), Recharts, Lucide icons, Error Boundary wrapper for crash-safe UI.
- **Config**: Environment variables via [backend/.env.example](backend/.env.example); default LLM provider is Ollama (`OLLAMA_MODEL=llama3.1:latest`).

## ðŸ“¦ Installation & Setup

### Prerequisites
- Python 3.10+
- Node.js 18+
- (Optional) Ollama running locally for default LLM

### Quick Start
```bash
chmod +x start.sh
./start.sh
```
This will set up the Python venv, install backend/frontend deps, run FastAPI on http://localhost:8000 and Vite on http://localhost:5173.

### Configuration
1) Copy the sample env: `cp backend/.env.example backend/.env`
2) Set `LLM_PROVIDER` to `ollama` (local) or `gemini` (cloud). For Gemini, add your `GEMINI_API_KEY`.
3) Optional thresholds and loop intervals can be tuned in `.env`.

## ðŸ§­ Using the POC
- **Monitor**: Foundation dashboard shows live OEE, power, and Pareto data.
- **Detect**: Anomaly dashboard shows historical charts; red points or event rows are clickable to open diagnosis.
- **Diagnose**: Guided Diagnosis chat auto-injects anomaly context; chats are persisted per anomaly and retrievable via `/api/anomaly/events/{id}/chat`.

## ðŸ”Œ Key APIs
- `GET /api/anomaly/events` â€“ latest anomalies (persisted to SQLite)
- `GET /api/anomaly/events/{id}` â€“ single anomaly with details
- `GET /api/anomaly/events/{id}/chat` â€“ chat history for an anomaly
- `POST /api/anomaly/events/{id}/chat` â€“ append chat message (role, content)
- `POST /api/diagnosis/analyze` â€“ run LLM diagnosis (supports `anomaly_id` to store chat)

## ðŸ”® Future Roadmap
See [ROADMAP.md](ROADMAP.md) for the planned evolution (Predictive Maintenance, Energy Analytics, CV Quality, Auto-tuning, Digital Twin, etc.).
